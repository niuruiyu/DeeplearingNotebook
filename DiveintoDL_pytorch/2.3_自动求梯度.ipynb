{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9b72934-7731-4099-9106-ae6c7bc71cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002A5D5F667D0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#用的是pytorch提供的autograd包\n",
    "#如果将tensor对象的requires_grad属性设为True\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n",
    "# grad_fn用来存储梯度函数，记录生成该tensor的运算操作\n",
    "#如果tensor时通过某些运算从其他tensor衍生来的，grad_fn会存储具体的运算类型（AddBackward……）\n",
    "# 如果tensor是初始化创建的，那么grad_fn就是None\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)  # 输出: <AddBackward0 object at 0x7f...>\n",
    "#定义：像x这种直接创建的称为“叶子节点”，“叶子节点”对应的grad_fn是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26ac0c8b-dc32-4184-a3a4-ddd8d9f9e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      " tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MulBackward0 object at 0x000002A5D5F666B0>\n",
      "<MeanBackward0 object at 0x000002A5D5F85B10>\n"
     ]
    }
   ],
   "source": [
    "# 进行一些更复杂的操作\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,'\\n',out)\n",
    "print(z.grad_fn)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db32e4a-503c-47a5-9401-27e121168e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "tensor(260.0344, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x000002A59CBD5E70>\n"
     ]
    }
   ],
   "source": [
    "#可以通过requires_grad_()采用inplace的方法来改变requires_grad这个属性\n",
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "#修改requires_grad属性\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1d5c36-ae40-46d3-825b-158127f77c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#总结上面一小节：\n",
    "#如果将tensor的requires_grad属性设为True，那么就可以追踪再上面的所有操作\n",
    "#grad_fn可以存储这个tensor的梯度函数，叶子节点是None，具体看这个tensor是经过什么操作得来的\n",
    "#requires_grad_(True/False)可以对tensor的requires_grad属性进行修改，使用的是inplace操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ed1fa97-16eb-4072-bab8-f6323e02e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#由上面的out\n",
    "print(out)\n",
    "#out是一个标量，所以调用backward()时不需要指定求导变量（无参数）\n",
    "out.backward()#out.backward(torch.tensor(1.))\n",
    "#backward(),计算所有叶子结点的梯度，是自动求导机制的关键入口（路线是随着tensor的grad_fn所构建的计算图，来反方向遍历所有运算节点\n",
    "# 最后将计算出的梯度存储到叶子节点的grad属性中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be7ab4a9-1d1c-475d-88f6-9c973e4b38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)#得到out关于x的梯度，也就是out(x)这个函数的导数\n",
    "#其实根据上面我们的叶子节点就只有x\n",
    "print(x.is_leaf)\n",
    "print(out.is_leaf)\n",
    "#梯度grad属性存在叶子节点中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b59c54bb-8dce-4212-9f19-95df30295d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入“雅可比矩阵”的概念，如果有一个“函数值和自变量都是向量”的函数，那么“函数值y关于自变量x的梯度”就是一个雅可比矩阵\n",
    "#torch.autograd这个包就这是用来计算一些雅可比矩阵的乘积的\n",
    "\n",
    "#还要注意：“grad在反向传播过程中是会累加的，所以每一次反向传播之前，要把之前的梯度清零，不然每一次运行都会累加之前的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5f5aee4-e027-44d9-a7e5-8640b3238a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n"
     ]
    }
   ],
   "source": [
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "#注意grad是累加的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1864ac71-51c0-4533-bed2-4851c0e776dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "out3 = x.sum()\n",
    "x.grad.data.zero_()#梯度置零\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6ed9541-cbc0-4927-a600-dbdecd516936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前面可以看到由于out都是标量（只有一维度），所以backward里面不用传入参数\n",
    "#但是如果是高维度的tensor，backward里面要传入参数，指定梯度权重（grad_tensors)\n",
    "#因为我们只允许标量对张量求导，求导的结果是和x同型的tensor\n",
    "#传入的参数w（是和out同型tensor）是如何将out变成标量然后再对x求导呢？\n",
    "#先引入一个中间变量l\n",
    "#l = torch.sum(out*w)\n",
    "#那l就是一个标量\n",
    "#然后求l对于x的导数就是最后的结果\n",
    "#下面是一个实际的example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d2111a3-98aa-40f1-8edd-cae85373ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0,4.0],requires_grad=True)\n",
    "y = 2*x\n",
    "z = y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f67094f-727d-44f5-a02b-879d2075fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1.0,0.1],[0.01,0.001]])\n",
    "print(w.shape == z.shape)#w的形状和z相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "196c5692-7e85-4042-8129-eb6b5af2b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "#求梯度\n",
    "z.backward(w)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcd1e9a8-101a-4a95-b0af-9cdb1ecaad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "#下面是一个中断梯度追踪的例子\n",
    "x = torch.tensor(1.0,requires_grad=True)\n",
    "y1 = x**2\n",
    "with torch.no_grad():#用这个把不想被追踪的操作代码包裹起来，中断梯度追踪\n",
    "    y2 = x**3\n",
    "y3 = y1+y2\n",
    "print(x.requires_grad)\n",
    "print(y1,y1.requires_grad)\n",
    "print(y2,y2.requires_grad)\n",
    "print(y3,y3.requires_grad)\n",
    "#y2没有grad_fn，而且requires_grad是False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0240e15-43dd-4995-aa8d-bc37b103610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "#求一下y3对x的梯度\n",
    "y3.backward()\n",
    "print(x.grad)#得到的结果是x**2的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c228538b-f146-4d47-8b4b-fa817695d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#如果想要修改tensor的数值，但是又不想被autograd记录，可以对tensor.data进行操作\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "print(x.data)\n",
    "print(x.data.requires_grad)\n",
    "#x.data是和x一摸一样的tensor\n",
    "#但是x.data的requires_grad是false，也就是独立于计算图之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7824861d-1c93-4379-9ec0-73047441303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "y = 2*x\n",
    "x.data *=100#只修改了x的值但是不会影响原来的梯度传播\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37c3a7-68a3-4711-82f0-13dfa68ccec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
