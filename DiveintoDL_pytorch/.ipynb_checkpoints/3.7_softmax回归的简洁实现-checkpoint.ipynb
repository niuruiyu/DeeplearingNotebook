{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579db6b3-be20-43c8-a554-2353ac34d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "import d2lzh_pytorch as d2l\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca8649-26a2-44df-bfb1-ae5bc7bc9b97",
   "metadata": {},
   "source": [
    "## 1.导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c712548-771c-4682-b5ca-25d65b2b926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里仍然和书中有差距，因为我没有把书中的函数放到d2l文件中，这里仍然是直接利用dataset和dataloader实现\n",
    "batch_size = 256\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root = '~/DataSets/FashionMNIST',train = True,download = True,transform = transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root = '~/DataSets/FashionMNIST',train = False,download = True,transform = transforms.ToTensor())\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 0\n",
    "else:\n",
    "    num_workers = 4\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train,batch_size,shuffle=True,num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test,batch_size,shuffle=True,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a377c-077c-4158-a1cf-40c32548342b",
   "metadata": {},
   "source": [
    "## 2.定义和初始化模型\n",
    "在softmax部分的输入和输出都是全连接层，但是我们的数据集本质上是（batch_size,1,28,28)，所以要先转换格式，变成（batch_size,784）的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ecbcea4-ba51-4a94-a293-379e931e84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x.view(x.shape[0],-1))\n",
    "        return y\n",
    "#模型实例化\n",
    "net = LinearNet(num_inputs,num_outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3028def-12d6-483b-a08f-b99f19c17ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3973bf92-d83d-47eb-b5c5-1f73f25778d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化参数\n",
    "init.normal_(net.linear.weight,mean=0,std=0.01)\n",
    "init.constant_(net.linear.bias,val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006390cb-a091-4a9c-9e0d-117506e79eaf",
   "metadata": {},
   "source": [
    "## 3.定义交叉熵损失函数，和优化算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d7eb78f-7a93-4f27-be3b-85e68f1c01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e83b9d-199d-4a2d-b16c-75b1dfea7988",
   "metadata": {},
   "source": [
    "## 4.训练模型\n",
    "因为原文这里放到了d2l里面，我这里有手动实现了一次方便熟悉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "476f8f6a-576c-4a95-ab83-13afd9d29711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20128\\3261789000.py:23: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  print('epoch %d,loss%.4f,train acc %.3f,test acc %.3f'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss0.0022,train acc 0.814,test acc 0.798\n",
      "epoch 2,loss0.0021,train acc 0.824,test acc 0.804\n",
      "epoch 3,loss0.0020,train acc 0.831,test acc 0.818\n",
      "epoch 4,loss0.0019,train acc 0.837,test acc 0.812\n",
      "epoch 5,loss0.0019,train acc 0.840,test acc 0.825\n"
     ]
    }
   ],
   "source": [
    "def train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params=None,lr =None,optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat,y).sum()\n",
    "            #先梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            #反向传播\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params,lr,batch_size)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            train_loss_sum+=l\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = d2l.evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d,loss%.4f,train acc %.3f,test acc %.3f'\n",
    "              %(epoch+1,train_loss_sum/n,train_acc_sum/n,test_acc))\n",
    "#因为前面在net那里进行了初始化参数，在loss这里进行了lr设置，所以这里直接设置为了None\n",
    "train_ch3(net = net,train_iter=train_iter,test_iter=test_iter,loss=loss,num_epochs=num_epochs,batch_size=batch_size,params=None,lr=None,optimizer=optimizer)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5d20e-e70c-4df0-aea2-c4fe08e111b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
