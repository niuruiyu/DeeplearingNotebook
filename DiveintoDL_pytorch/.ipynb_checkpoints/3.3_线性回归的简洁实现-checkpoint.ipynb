{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848bba3d-4248-424c-9b1d-3cd0c2b54744",
   "metadata": {},
   "source": [
    "## 1.生成数据集  \n",
    "和上一节的步骤相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ee705e-7191-4bdc-9e10-493134e03303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2,-3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0,1,(num_examples,num_inputs)),dtype=torch.float)\n",
    "labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b\n",
    "labels += torch.tensor(np.random.normal(0,0.01,size = labels.size()),dtype = torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0fa52-1e54-489b-ac77-34f76a08587d",
   "metadata": {},
   "source": [
    "## 2.读取数据\n",
    "使用pytorch内的data包来实现。  \n",
    "但是因为“data”经常做变量名，所以习惯上as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02fac5fc-8e20-4b67-aad1-d539cd046761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "batch_size = 10\n",
    "#将features和labels组合\n",
    "dataset = Data.TensorDataset(features,labels)\n",
    "#随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19956ba6-3d98-42cb-9284-a8d140d38938",
   "metadata": {},
   "source": [
    "**TensorDataset()**\n",
    "可以将多个张量组合成一个数据集对象  \n",
    "**DataLoader()**\n",
    "将Dataset包装成可迭代的对象，方便按批次获取数据，还可以打乱数据，多线程加载……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2071f1d-d519-49a6-b381-7c3746e0dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5641,  0.1158],\n",
      "        [-0.5645, -0.4775],\n",
      "        [-1.6263, -1.1108],\n",
      "        [ 0.2507, -0.1004],\n",
      "        [ 1.1427,  0.4148],\n",
      "        [-0.1094, -0.1142],\n",
      "        [ 1.1159,  0.4917],\n",
      "        [-0.7893,  0.4742],\n",
      "        [-0.6442, -0.8297],\n",
      "        [-0.6882,  1.8519]]) tensor([ 4.9324,  4.6865,  4.7378,  5.0364,  5.0682,  4.3628,  4.7519,  1.0018,\n",
      "         5.7246, -3.4772])\n"
     ]
    }
   ],
   "source": [
    "#打印一组数据\n",
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d67f5-64c7-4e39-8d45-234a0133727a",
   "metadata": {},
   "source": [
    "## 3.定义模型\n",
    "torch中提供了很多预定义的层  \n",
    "要导入nn（neural networks）模块，里面包含了大量神经网络的层\n",
    "nn的核心数据结构是Module，在实际应用中，最常见的做法是继承nn.Module，撰写自己的网络  \n",
    "一个nn.Module实例应该包含“层”和返回输出的forward方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c95b7ddf-34c9-4710-8985-d45db4eaad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_features):\n",
    "        super(LinearNet,self).__init__()\n",
    "        #n_features是输入的神经元个数，1是输出神经元个数\n",
    "        self.linear = nn.Linear(n_features,1)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b70fb9d7-619f-4923-9b19-767a6477be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#模型实例化\n",
    "net = LinearNet(num_inputs)\n",
    "#打印出网络的结构\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccff671-608d-4a4d-88c2-0ee82c8af08c",
   "metadata": {},
   "source": [
    "还有其他搭建网络的方法  \n",
    "写法一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07965faa-ef18-44ed-ab12-a731f5a9afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs,1)\n",
    "    #其他层\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b872818-bfb7-4d97-bdcb-1e95a6c1a877",
   "metadata": {},
   "source": [
    "写法二："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29fc0531-0095-4939-9569-cae38995237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add_module('linear',nn.Linear(num_inputs,1))\n",
    "#下面可以加别的层\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e5487-3ade-492b-99f2-aa627ec725da",
   "metadata": {},
   "source": [
    "写法三："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fb02970-1095-44d0-bce5-d0e16db5acd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "    ('linear',nn.Linear(num_inputs,1))\n",
    "    #更多层\n",
    "]))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35fef321-41a1-4a12-9a2d-7c46e00a1b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(net[0])#第一层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bf0d0-4c3f-40ad-9ea2-7d92cbd11e0b",
   "metadata": {},
   "source": [
    "可以通过net.parameters()来查看模型的所有可学习参数，此函数将返回一个生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f5ffc67-c8c3-4e51-acd2-a04673ba1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.5104e-04, 6.5345e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3984], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68fa4e-0683-4cb4-8dcc-548823c3767b",
   "metadata": {},
   "source": [
    "以上是如何利用pytorch构建线性回归神经网络层，线性回归神经网络层又可以叫做全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b84aa8-e958-4cba-930b-c0850e0f93d4",
   "metadata": {},
   "source": [
    "## 4.初始化模型参数\n",
    "初始化线性回归模型中的权重和偏差\n",
    "pytorch在init（initializer）模块中提供了很多参数初始化的方法  \n",
    "\n",
    "可以通过init.normal_将权重参数每个元素初始化为随机采样于均值为0，标准差为0.01”的正态分布  \n",
    "\n",
    "bias会初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fbeac42-a2d8-433a-b12f-c0ba518a468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "init.normal_(net[0].weight,mean=0,std=0.01)\n",
    "init.constant_(net[0].bias,val=0)\n",
    "#或者可以直接修改bias的data：net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ade46c-da9c-4037-b252-4db5c197a16f",
   "metadata": {},
   "source": [
    "上面的初始化代码要求：net是个ModuleList或者Sequential实例时才可以\n",
    "所以，如果使用的构建网络代码是：\n",
    "```python\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_features):\n",
    "        super(LinearNet,self).__init__()\n",
    "        #n_features是输入的神经元个数，1是输出神经元个数\n",
    "        self.linear = nn.Linear(n_features,1)\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "```\n",
    "对应的参数初始化代码应该是：\n",
    "```python\n",
    "init.normal_(net.linear.weight,mean=0,std=0.01)\n",
    "init.constant_(net.linear.bias,val=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9171dc-97e6-47c7-a741-10c0946d4aa0",
   "metadata": {},
   "source": [
    "## 5.定义损失函数\n",
    "损失函数在nn模块中提供  \n",
    "这里使用MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b155aef1-3d75-4771-8db4-a1ee0194e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff07f23-d56a-4680-8375-0f5273cb2977",
   "metadata": {},
   "source": [
    "## 6.定义优化算法\n",
    "优化算法由torch.optim模块提供，像是SGD，Adam，RMSProp等\n",
    "此处使用SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b66747e-e1fc-48f4-b307-f8dcf28a1ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.03\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.03)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6974578-ff0e-409f-b2ad-1e2549560c3f",
   "metadata": {},
   "source": [
    "还可以给不同的子网络设置不同的学习率\n",
    "ex：\n",
    "```python\n",
    "optimizer = optim.SGD([\n",
    "    {'params':net.subnet1.parameters()}#第一层的学习率就是0.03\n",
    "    {'params':net.subnet2.parameters(),'lr':0.01}\n",
    "]lr=0.03)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90310c9a-b60b-4c3f-a039-94e8597ac5c8",
   "metadata": {},
   "source": [
    "不想让学习率是一个固定的常数时，有两种做法：  \n",
    "方法一：修改optimizer.param_groups中对应的学习率\n",
    "```python\n",
    "for param_group in optimizer.para,_groups:\n",
    "    param_group['lr']*=0.1\n",
    "```\n",
    "方法二：新建新的optimizer，但是对于使用动量的优化器会丢失动量等状态信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec655d-d284-4c88-867b-347b33ce58d8",
   "metadata": {},
   "source": [
    "## 7.训练模型\n",
    "通过调用optim实例中的step函数迭代模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e8ff7cd-3f1c-499d-ac87-f2287bf655c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss: 0.000253\n",
      "epoch 2,loss: 0.000046\n",
      "epoch 3,loss: 0.000072\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        # 每一个batch梯度都要清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d,loss: %f' % (epoch,l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34142c-48ef-4d45-bf86-85580a8dd35c",
   "metadata": {},
   "source": [
    "学习到的参数和真实值的对比："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ade2a1f4-2e45-4cfa-b701-baf17283e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] \n",
      " Parameter containing:\n",
      "tensor([[ 2.0005, -3.3989]], requires_grad=True)\n",
      "4.2 \n",
      " Parameter containing:\n",
      "tensor([4.2003], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dense = net[0]\n",
    "print(true_w,'\\n',dense.weight)\n",
    "print(true_b,'\\n',dense.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe691a9-d815-4b75-9d8e-5ad4d53a7c40",
   "metadata": {},
   "source": [
    "## 8.SUMMARY\n",
    "此节使用pytoch完成了线性回归模型的训练  \n",
    "这一节中更深一步的了解到了pytorch中关于模型训练模型构建的模块和函数\n",
    "### **1.torch.utils.data**\n",
    "提供了有关数据处理的函数\n",
    "#### **（1）Data.TensorDataset()**\n",
    "将多个tensor组合起来形成新的数据集（常用于合并特征和labels）\n",
    "#### **（2）Data.DataLoader()**\n",
    "将数据集划分为batch，可以用来迭代，迭代时，一个循环是一个batch  \n",
    "我们构建的模型训练输入时也只以batch为单位进行输入  \n",
    "如果是以一个样本作为单位：\n",
    "可以用下面的代码来添加一维，其实就是把他的一个样本看成一个batch，batch_size=1\n",
    "```python\n",
    "input.unsqueeze(0)\n",
    "```\n",
    "### **2.torch.nn**\n",
    "定义大量的神经网络层  \n",
    "这里不多说了，可以具体看我的projects目录下的实战代码中的model.py模型构建的具体代码\n",
    "### **3.torch.nn.init**\n",
    "定义了各种初始化方法\n",
    "#### **（1）init.normal_(参数，mean=，std=)**\n",
    "正态分布\n",
    "#### **（2）init.constant_(参数，val=)**\n",
    "常数设置\n",
    "### **4.torch.optim**\n",
    "提供了很多常用的优化算法\n",
    "#### **optimizer = optim.SGD(net.parameters(),lr =)**\n",
    "这里具体只给出了SGD（随机梯度下降法）的使用，主要是学习率的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea85fc-36e8-47e9-848d-792f7aa377cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
